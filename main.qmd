---
title: "Retrieving and organising the tweets"
format: html
editor: visual
---

# 0. Set up

These are the R libraries we need

```{r, message = FALSE}
library(tidyverse) # main functions
library(rtweet) # access Twitter API
library(readr) # reading in files
library(janitor) # cleans up datasets
```

# 1. List tweets

Getting the link to the first and last Tweets in each thread allows for the whole thread to be found.

This list was created by hand. The Google Sheet used has 7 columns:

-   Title (based on the master list on Twitter)

-   Short Link (this came from a quick API pull, but they aren't the links we need to actually pull the fics)

-   Short ID (used for naming folders and paths, should be short-ish and have no spaces or unusual characters)

-   FIRST (the link to the first tweet in the thread, it should be formatted like this: <https://twitter.com/taehyikes/status/1479132323547332611> as the numbers at the end are important)

-   LAST (like above but the last tweet to be included in the thread)

-   Added by (optional, just to track who contributed)

```{r, message = F}
thread_list_raw <- read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vT7N5QJlcEY09a_5onomH6ZW38Nb_vb9pGBjEghJFXSnPzRjF2Dz1qx7l4iSFcPTFcCA0VWAyF36xlk/pub?gid=0&single=true&output=csv")

```

# 2. Retrieve Tweets

Which ones have already been done? Which are left to do?

```{r}
# Checks which files already have an .RDS file created
completed <- tibble(short_id = list.files(pattern = ".rds")) |> 
  mutate(short_id = str_remove(short_id, ".rds"))

to_do <- thread_list_raw |> 
  clean_names() |> 
  filter(!is.na(short_id)) |> 
  filter(!(short_id %in% completed$short_id))
  
```

## Access the API

A Twitter Developer account is needed to do this

```{r, eval = FALSE}
# Authentication for rtweet not shown as keys are private
# Anyone re-running or replicating this can learn more here
vignette("auth", package = "rtweet")

```

```{r, message=F, eval=FALSE}
author <- "taehyikes"

for(i in 1:nrow(to_do)){
  
  # Name to use for the paths
  short_id = to_do$short_id[i]
  
  # Created folder
  if(!dir.exists(short_id)) dir.create(short_id)
  
  # Use first and last tweet IDs to get all other IDs for the thread
  
  first = str_remove(to_do$first[i], 
                     str_c("https://twitter.com/", 
                     author, "/status/"))
  last = str_remove(to_do$last[i], 
                     str_c("https://twitter.com/", 
                     author, "/status/"))

  ids = last

  # This will break if a thread is somehow longer than 10,000 tweets
  for(j in 1:10000){
    lookup = lookup_tweets(ids[j])
    ids[j+1] = lookup$in_reply_to_status_id_str
  
  if(first == lookup$in_reply_to_status_id_str) break
  }
  
  my_seq <- 1:ceiling(length(ids)/100)

  
  get_urls <- function(x){
    lookup_tweets(x,
                  retryonratelimit = T,
                       parse = F
                       )[[1]]$extended_entities$media
  }
  
  slowly_get_urls <- slowly(get_urls, 
                            rate = rate_delay(pause = 1),
                            quiet = T)
  
  image_urls = list()
    for(batch in my_seq){
      image_urls[(batch*100-99):(batch*100)] = tibble(ids[(batch*100-99):(batch*100)]) |> 
      map(~slowly_get_urls(.))
    }
  
  image_urls_clean  <- image_urls |> 
    bind_rows() |> 
    distinct()
  
  
  # Row per image
  extract_images <- image_urls_clean |> 
    select(expanded_url, media_url_https) |> 
    mutate(id_str = str_extract(expanded_url, "(?<=status/)(.*)(?=/photo)")) |> 
    select(id_str, media_url_https)
  
  
  # For text
  get_text <- function(x){
    lookup_tweets(x, retryonratelimit = T)
  }
  
  full_text <- tibble(ids) |> 
    map(~get_text(ids)) |> 
    bind_rows() |> 
    select(created_at, id_str, in_reply_to_status_id_str, full_text) |> 
    filter(id_str %in% ids) |> 
    arrange(created_at) |> 
    mutate(post_id = row_number()) |> 
    full_join(extract_images, by = "id_str") |> 
    group_by(created_at) %>% 
    mutate(photo_id = row_number()) |>
    mutate(ids_n = length(ids),
           short_id = short_id) |>
    mutate(title = case_when(
      ids_n < 10 ~ sprintf("/%01d—%01d.jpg", post_id, photo_id),
      ids_n < 100 ~ sprintf("/%02d—%01d.jpg", post_id, photo_id),
      ids_n < 1000 ~ sprintf("/%03d—%01d.jpg", post_id, photo_id),
      TRUE ~ sprintf("/%04d—%01d.jpg", post_id, photo_id)
      )) |> 
    mutate(path = str_c(short_id, title))
  
  ## Note to self: EDIT HERE
  saveRDS(full_text, str_c(short_id, ".rds"))
  
  safe_download <- safely(~ download.file(.x , .y, mode = "wb"))
  
  map2(full_text$media_url_https, full_text$path, safe_download)
}

```

# 3. Push to GitHub

Can be done in the terminal

```{bash}
git add -A
git commit -m "Adding summary update"
git push
```
